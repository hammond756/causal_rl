\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage{biblatex}
\bibliography{references}

% produce boxed 'notes' in text
\usepackage[dvipsnames]{xcolor} %frame color
\usepackage{framed} % for defining framed environment
\newenvironment{formal}{
    \def\FrameCommand{{\color{YellowOrange}\vrule width 2pt}\hspace{2pt}}
    \MakeFramed{\advance\hsize-\width}
    \vspace{2pt}\noindent\hspace{-7pt}\vspace{3pt}
    }{\vspace{3pt}\endMakeFramed}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath, amsfonts}
\usepackage{tikz}
\usepackage{color}
\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{lineno}
\usepackage{algorithm2e}

\linenumbers

\newcommand{\red}[1]{{\color{red}{#1}}}

% redefine strings for subsubsection references in autoref package: https://tex.stackexchange.com/questions/177007/autoref-showing-subsection-and-subsubsection
\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname

\begin{document}

\section{Method}

\subsection{Environment}

As is common in reinforcement learning problems, data is generated by interacting with an environment. In the case of this research, the environment is governed by a structural causal model. The interface with the environment in designed in such a way that the agent can obtain observations from three different data modalities, based on the type of action it performs. These three levels of data correspond to Pearl's causal hierarchy \cite{pearl2009causality}.

By default, if the agent does not interact with the environment, the environment will return data sampled from the joint distribution of the causal variables $P(V)$. Alternatively, by performing an action the agent changes the distribution of the environment to the corresponding interventional distribution $P(V|do(X=\mathbf{x}))$ and will observe new data points accordingly. Finally, the environment can also be queried for counterfactuals. This differs from the other two data modalities because a counterfactual query is posted after an observation has been made. Such a counterfactual distribution is formally denoted as $P(V|do(X=\mathbf{x}), U=\mathbf{u})$. The set of allowed interventions is the power set of the causal variables, denoted as $X = \mathcal{P}(V)$. Depending on the task the set of allowed interventions may limited to a stricter subset of $V$.

Since the causal mechanisms governing $V$ are are deterministic, the joint distribution is determined by the distribution on the noise variables $P(U)$. In theory any distribution can be chosen. However, in other causal discovery methods, the noise distribution has shown to affect the algorithm's ability to recover the causal model \cite{TODO}. For the environments considered in this research, it holds that the noise variables are all independent and identically distributed (i.i.d.).

\subsection{Agent}

On a high level, the agent is made up of two parts. An intervention policy and a causal model. The intervention policy determines which actions to take, and therefore determines the data the causal model observes. The causal model is an internal representation of the environment the agent is deployed in and is tasked with predicting the counterfactual for each observation under a specific intervention.

\subsubsection{Intervention Policy}

To decide which intervention to perform, the agents has an intervention policy. This policy is a function $\pi_\phi : V \rightarrow X$ that maps an observation to a probability distribution over allowed interventions. This policy is optimized to maximize the prediction loss incurred by the causal prediction model, using a form of adversarial reinforcement learning. An example of adversarial RL has been demonstrated by Pathak et al. \cite{pathak2017curiosity}. Their paper outlines an RL agent that learns a policy based on an intrinsic reward signal, provided by the prediction error of a world model.

Given an observation $\mathbf{v}$  from the environment, the policy calculates a distribution over actions. The action it will perform, is sampled from this distribution. In similar vain to \cite{pathak2017curiosity}, the RL agent's reward signal does not come from the environment, but from another module inside the agent. In this case, the reward signal is inversely proportional to the loss of predicting the counterfactual. This reward is supposed to represent the difficulty the causal model has with predicting the outcome of the observed state and selected intervention. The policy learns to assign high probability to difficult state-intervention pairs.

The reinforcement learning method that is used is REINFORCE \cite{sutton2000policy}. For details on the general method, see \autoref{sec:pgt}. The objective is as follows


\begin{align}
    \nabla_\phi J(\phi) &= \mathbb{E}_\pi \left[
    G_t \nabla_\phi\log\pi_\phi(\mathbf{v}^{(t)}) \right]\label{eq:rl_obj_grad}
\end{align}

where $G_t = \sum_{t=1}^T \gamma^{t-1} r_t$. This equation can be simplified by making use of the fact that the environments don't have transition probabilities (or uniform transition probabilities) and problem of selecting an intervention is essentially a bandit problem \cite{sutton2018reinforcement}. In this setting, selecting one intervention (i.e. bandit arm) corresponds to an episode. So \autoref{eq:rl_obj_grad} can be written as

\begin{align}
    \nabla_\phi J(\phi) &= \mathbb{E}_\pi \left[
    r_t \nabla_\phi\log\pi_\phi(\mathbf{v}) \right]
\end{align}

Like described above, the reward at each time step is negatively proportional to the prediction error of the causal model. Specifically

\begin{equation}
    r_t = - MSE\left(APC(\mathbf{v}^{(t)}, do(X=\mathbf{x})), \mathbf{v}^{(t)}\right)
\end{equation}\label{eq:reward}

where APC is the causal model that is described in the next section and MSE is the mean squared error between the output and the target. Attentive readers will notice that the reward is itself a function of the policy. The indirect effect of $\phi$ on $r$ will be ignored. This corresponds to the method as described in \cite{pathak2017curiosity}. For this reason, the intervention in \autoref{eq:reward} is denoted as $do(X=\mathbf{x})$ and not as $\pi_\phi(\mathbf{v}^{(t)})$.

\subsubsection{Abductor-Predictor Causal Model}

The Abductor-Predictor Causal (APC) model consists of two modules: the Abductor and the Predictor. These modules represent two of the steps that form the recipe to answering counterfactual question. The Abductor infers the noise vector that underlies the incoming observation. The Predictor represents the actual causal model. The predictor produces a (hypothetical) observation, given noise vector that was inferred by the Abductor.

Formally, the Abductor is a function $f_\theta : V \rightarrow U$, parameterized by $\mathbf{\theta}$ that maps an observation to a noise vector. This module can be implemented by any differentiable function. Because one of the goals of this research is to learn an explicit causal model, the implementation of the Predictor needs to take that into account. If the environment is a linear causal model, then the Predictor is a function $f_\mathbf{B} : U \times X \rightarrow V $ where $X$ is the space of allowed interventions and $\mathbf{B}$ represents the learned causal model.

The output of the Predictor is computed as follows:

\begin{equation}
    \mathbf{\hat{v}}' = \sum_{d=1}^D \left(\mathbf{B}_{i=0}\right)^{d-1} \mathbf{\hat{u}}_{i=x}
\end{equation}

where superscript denotes the matrix power and the subscript ${i=x}$ is a shorthand for masking the $i$'th row with the value $x$. Masking the weight matrix and the noise vector ensures that the intervention $do(X_i=x)$ is applied to the model. Intuitively, each element of the sum computes the contribution of each noise variable to  its $d$'th generation descendants. The first iteration, where $d=1$, is the direct effect of the noise on the observation. The next iteration represents the contribution of the noise to the children of the corresponding variable, and so on. The next iteration computes the effect of each noise variable on its grandchildren, and so on. This has to be repeated $D$ times because that is the longest possible directed path in a causal model.

The benefit of formulating the Predictor in this way is that the causal ordering of the variables is not explicit. The ordering of the variables is encoded by the indices of the non-zero weights in $\mathbf{B}$. The following matrices encode the same causal model, under different causal orderings:

\begin{equation}
    \mathbf{B}_1 = \begin{bmatrix}
        0 & 0 & 0 \\
        a & 0 & 0 \\
        b & c & 0
    \end{bmatrix} \quad \mathbf{B}_2 = \begin{bmatrix}
        0 & c & b \\
        0 & 0 & a \\
        0 & 0 & 0
    \end{bmatrix}
\end{equation}

where $p_1 = [1, 2, 3]$ and $p_2 = [3, 2, 1]$. In addition to the causal ordering, the model can represent cycles. While these are by definition not present in the environment, this means that the model can be initialized as a fully connected graph that is pruned during training. Fully assembled, the module looks as follows: $\hat{\textbf{v}}' = f_\mathbf{B}(f_\theta(\textbf{v}, do(X_i=x))$. Here $\mathbf{B}$ and $\theta$ represent the weights of the Predictor and Abductor respectively.

\subsection{Training}

\begin{table}[h]
\begin{tabular}{ll}
\textbf{Parameter} & \textbf{Description}                                                                                                     \\
T            & The number of environment interactions                                                                                   \\
$\pi$         & The intervention policy to use \\
$x$ & The value that is used for the intervention $do(X=\mathbf{x})$                                                                  \\
$(\eta_a, \eta_p)$                 & Learning rates for abductor and predictor respectively                                                                                                      \\
$\eta_\pi$ & Learning rate for policy, if required \\
$(\lambda_1, \lambda_2)$         & Regularization coefficients
\end{tabular}
\caption{Hyperparameters}\label{table:hyper_params}
\end{table}

Depending on the policy, there are two modes of training. In the simplest case, where there is a fixed policy, training is a standard supervised learning problem that minimizes $\mathcal{L(\hat{\textbf{v}}', \textbf{v}')}$. The training data consists of pairs of observations and counterfactuals $(\mathbf{v}, \mathbf{v'})$ that are generated by sampling from a linear SEM. The other mode involves updating a parameterized policy during training. In addition to minimizing $\mathcal{L}$, the policy that generates the interventions is updated using a policy gradient reinforcement learning algorithm. Adding the RL component to the training loop changes the distribution of the data during training. The procedure in \autoref{alg:training_loop} summarizes both training modes.

% \begin{algorithm}
% \SetAlgoLined
% \KwResult{Estimated causal model $\mathbf{\phi}$ }
%  initialize: $\mathbf{\phi}$, $\mathbf{\theta}$ \;
%  \For{$t = 1...T$}{
%   $\mathbf{v} \sim M$  \;
%   $i \sim \pi()$ \;
%   $\mathbf{v'} \gets M^{do(X_i=x)|V=\mathbf{v}}$ \;
%   $\hat{\mathbf{v}}' \gets f_\phi(f_\theta(\textbf{v}), \text{do}(X_i=x))$ \;
%   $\phi \gets \phi + \eta_\phi \nabla_\phi \mathcal{L}(\mathbf{\hat{\mathbf{v}'}, \mathbf{v}'})$ \;
%   $\theta \gets \theta + \eta_\phi \nabla_\theta \mathcal{L}(\mathbf{\hat{\mathbf{v}'}, \mathbf{v}'})$ \;
%  }
%  \caption{Fixed policy training}\label{alg:simple_training}
% \end{algorithm}

\begin{algorithm}
\SetAlgoLined
\LinesNumbered
\KwResult{Estimated causal model $\mathbf{B}$ }
 initialize: $\mathbf{B}$, $\mathbf{\theta}$, $\mathbf{\phi}$\;
 \For{$t = 1...T$}{
  $\mathbf{v} \sim M$  \;
  $i \sim \pi_\phi(\mathbf{v})$ \;
  $\mathbf{v'} \gets M^{do(X=\mathbf{x})|V=\mathbf{v}}$ \;
  $\hat{\mathbf{v}}' \gets f_\mathbf{B}(f_\theta(\textbf{v}), do(X=\mathbf{x}))$ \;
  $\mathbf{B} \gets \mathbf{B} + D \eta_p \nabla_\mathbf{B} \mathcal{L}(\mathbf{\hat{\mathbf{v}'}, \mathbf{v}'})$ \;
  $\theta \gets \theta + D \eta_a \nabla_\theta \mathcal{L}(\mathbf{\hat{\mathbf{v}'}, \mathbf{v}'})$ \;
  \If{$\pi$ is adaptive}{
   $\phi \gets \phi + \eta_\pi \nabla_\phi J(\phi)$ \;
   }
 }
 \caption{Training loop}\label{alg:training_loop}
\end{algorithm}

During training, the agent has two interactions with the environment $M$. Initially, the agent observes a state $\mathbf{v}$. Then, based on the state and the policy, the agent proposes an intervention for which it wants to observe the counterfactual observation $\mathbf{v}'$. The agent uses the observation and the intervention to predict the alternative state $\mathbf{\hat{v}}'$.

The supervised learning objective is a combination of the predictive power of the model and a combination of regularizers. The predictive power is quantified by the mean squared error (MSE) between the true counterfactual and the model prediction. Both regularizers implement the Lasso \cite{tibshirani1996regression}. This metric penalizes non-zero coefficients and acts as a variable selection mechanism. In the first case, the Lasso is applied directly to $\mathbf{B}$, the weights of the Predictor. In the second case, it is applied to the $D^{th}$ degree matrix power of $\mathbf{B}$. The first acts as a variable selection mechanism, encouraging a sparse matrix. The second case acts as a penalty on cycles\footnote{
if $A$ is an acyclic $N \times N$ autocorrelation matrix, then $||A^N||_1=||0_{N,N}||_1=0.$ Only in special cases does this hold for a matrix containing cycles.
}. Both regularizers have their own regularization coefficient $\lambda_1$ and $\lambda_2$ respectively. The policy gradient objective function $J(\phi)$ is inversely proportional to the prediction error (MSE), to promote data that is hard for the Predictor.

\begin{equation}
    \mathcal{L(\hat{\textbf{v}}', \textbf{v}')} = \frac{1}{D} \sum_i^D (\hat{v}'_i - v'_i)^2 + \lambda_1 ||\textbf{B}||_{1} + \lambda_2 ||\textbf{B}^D||_1
\end{equation}

In addition to hyperparameters like the learning rates, regularization coefficients and intervention value, there are some higher level configurations that affect training. The policy, and whether it is adaptive or not, is one such example. The flexibility of the causal model can also be changed. In the most general case, $\mathbf{B}$ is initialized as a fully connected graph and the model is expected to capture both the weights and the causal ordering, and remove cycles. To facilitate learning, two constraints can be applied to the model. These constraints correspond to characteristics that hold for the true model and they are that the graph contains no self-loops and has a known causal ordering.

\printbibliography

\end{document}
